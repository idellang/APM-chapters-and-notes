---
title: "Chap19 - Exercises"
author: "Me"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#1
FOr the biomarker data, determine if the in between predictors correlation have an effect on the feature selection process
a - create initial filter of predctors that remove predictors to minimize amount of collinearity prior to modeling
b - refit recursive feature selection models
c - Did RFE change considerably. Which models are affected by multicollinearity

```{r}
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
```
```{r}
library(doParallel)
cl = makeCluster(8)
registerDoParallel(cl)
```

Baseline set of preds
```{r}
bl <- c("Genotype", "age", "tau", "p_tau", "Ab_42", "male")


## The set of new assays
newAssays <- colnames(predictors)
newAssays <- newAssays[!(newAssays %in% c("Class", bl))]
```

Decompose genotype to dummy vars
```{r}
predictors$E2 <- predictors$E3 <- predictors$E4 <- 0
predictors$E2[grepl("2", predictors$Genotype)] <- 1
predictors$E3[grepl("3", predictors$Genotype)] <- 1
predictors$E4[grepl("4", predictors$Genotype)] <- 1
genotype <- predictors$Genotype
```

Partition the data
```{r}
library(caret)
set.seed(730)
split <- createDataPartition(diagnosis, p = .8, list = FALSE)

adData <- predictors
adData$Class <- diagnosis

training <- adData[ split, ]
testing <- adData[-split, ]
```

Stat summaries
```{r}
predVars <- names(adData)[!(names(adData) %in% c("Class","Genotype"))]

## This summary function is used to evaluate the models.
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))

## We create the cross-validation files as a list to use with different
## functions

set.seed(104)
index <- createMultiFolds(training$Class, times = 5)

## The candidate set of the number of predictors to evaluate
varSeq <- seq(1, length(predVars)-1, by = 2)
```

The rfe() function in caret is used for recursive feature elimination. We setup control and train that use same CV folds. THe `ctrl` object will be modified
```{r}
ctrl <- rfeControl(method = "repeatedcv", repeats = 5,
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

fullCtrl <- trainControl(method = "repeatedcv",
                         repeats = 5,
                         summaryFunction = fiveStats,
                         classProbs = TRUE,
                         index = index)
```

Original correlation matrix
```{r}
predCor <- cor(training[, newAssays])
```

```{r}
library(RColorBrewer)
cols <- c(rev(brewer.pal(7, "Blues")), brewer.pal(7, "Reds"))
library(corrplot)
corrplot(predCor, order = "hclust", tl.pos = "n", addgrid.col = rgb(1,1,1,.01), col = colorRampPalette(cols)(51))
```

##a
```{r}
predictors_to_drop = findCorrelation(predCor, cutoff = .75)
newAssays[predictors_to_drop]
```
Drop these correlated preds
```{r}
newAssays = newAssays[-predictors_to_drop]
training = training[,-predictors_to_drop]
testing = testing[,-predictors_to_drop]
predVars = newAssays

```

Replot correlation matrix
```{r}
corrplot(cor(training[,newAssays]), order = "hclust", tl.pos = "n",addgrid.col = rgb(1,1,1,.01), col = colorRampPalette(cols)(51))
```
##b try to perform RFE

```{r}
ctrl$functions = rfFuncs
ctrl$functions$summary = fiveStats
```
```{r}
set.seed(721)
rf_rfe = rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', tol = 1e-12, rfeControl = ctrl)
rf_rfe
```

##LDA
```{r}
ctrl$functions = ldaFuncs
ctrl$functions$summary = fiveStats
set.seed(721)
lda_rfe = rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', tol = 1e-12, rfeControl = ctrl)
lda_rfe
```

#nb
```{r}
ctrl$functions = nbFuncs
ctrl$functions$summary = fiveStats
set.seed(721)
nb_rfe = rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', tol = 1e-12, rfeControl = ctrl)
nb_rfe
```


This option tells train() to run its model tuning suquentially. 
```{r}
cv_ctrl = trainControl(method = 'cv', verboseIter = F, classProbs = T, allowParallel = F)
```

Here, the caretFuncs allows model to be tuned at each iteration of feature selection
```{r}
ctrl$functions = caretFuncs
ctrl$functions$summary = fiveStats
set.seed(721)
svm_rfe = rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', rfeControl = ctrl, 
              ##arguments for train
              tuneLength = 12, preProc = c('center','scale'), trControl = cv_ctrl)
svm_rfe
```

Linear reg
```{r}
ctrl$functions = lrFuncs
ctrl$functions$summary = fiveStats
set.seed(721)
lr_rfe = rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', rfeControl = ctrl)
lr_rfe
```


KNN
```{r}
ctrl$functions <- caretFuncs
ctrl$functions$summary <- fiveStats
set.seed(721)
knn_rfe =  rfe(training[,predVars], training$Class, sizes = varSeq, metric = 'ROC', method = 'knn',
               tuneLength = 20, prepProc = c('center','scale'), trControl = cv_ctrl, rfeControl = ctrl)
knn_rfe
```


Extract x and y points using plot command

```{r}
p_lda = plot(lda_rfe)
p_rf = plot(rf_rfe)
p_nb = plot(nb_rfe)
p_lr = plot(lr_rfe)
```
Plot this in the same format 
```{r}
min_y = min( p_lda$panel.args[[1]]$y, p_rf$panel.args[[1]]$y, p_nb$panel.args[[1]]$y, p_lr$panel.args[[1]]$y)
max_y = max( p_lda$panel.args[[1]]$y, p_rf$panel.args[[1]]$y, p_nb$panel.args[[1]]$y, p_lr$panel.args[[1]]$y )
```

```{r}
par(mfrow=c(2,2))
plot( p_lda$panel.args[[1]]$x, p_lda$panel.args[[1]]$y, ylim=c(min_y,max_y), xlab="", ylab="", main="LDA" )
plot( p_rf$panel.args[[1]]$x, p_rf$panel.args[[1]]$y, ylim=c(min_y,max_y), xlab="", ylab="", main="RF" )
plot( p_nb$panel.args[[1]]$x, p_nb$panel.args[[1]]$y, ylim=c(min_y,max_y), xlab="", ylab="ROC", main="NB" )
plot( p_lr$panel.args[[1]]$x, p_lr$panel.args[[1]]$y, ylim=c(min_y,max_y), xlab="", ylab="ROC", main="LR" )
```

#2

Use same resampling process to evaluate penalized LDA model. How does the performance compare? Is the same variable selection observed in both models
```{r}
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
```

```{r}
library(doParallel)
cl = makeCluster(8)
registerDoParallel(cl)

```

```{r}
## The baseline set of predictors
bl <- c("Genotype", "age", "tau", "p_tau", "Ab_42", "male")

## The set of new assays
newAssays <- colnames(predictors)
newAssays <- newAssays[!(newAssays %in% c("Class", bl))]

## Decompose the genotype factor into binary dummy variables

predictors$E2 <- predictors$E3 <- predictors$E4 <- 0
predictors$E2[grepl("2", predictors$Genotype)] <- 1
predictors$E3[grepl("3", predictors$Genotype)] <- 1
predictors$E4[grepl("4", predictors$Genotype)] <- 1
genotype <- predictors$Genotype
```

```{r}
## Partition the data
library(caret)
set.seed(730)
split <- createDataPartition(diagnosis, p = .8, list = FALSE)

adData <- predictors
adData$Class <- diagnosis

training <- adData[ split, ]
testing <- adData[-split, ]

predVars <- names(adData)[!(names(adData) %in% c("Class","Genotype"))]

## This summary function is used to evaluate the models.
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))

## We create the cross-validation files as a list to use with different 
## functions

set.seed(104)
index <- createMultiFolds(training$Class, times = 5)

## The candidate set of the number of predictors to evaluate
varSeq <- seq(1, length(predVars)-1, by = 2)
```


```{r}
## The rfe() function in the caret package is used for recursive feature
## elimiation. We setup control functions for this and train() that use
## the same cross-validation folds. The 'Ctrl' object will be modifed several 
## times as we try different models

ctrl <- rfeControl(method = "repeatedcv", repeats = 5,
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

fullCtrl <- trainControl(method = "repeatedcv", repeats = 5,
                         summaryFunction = fiveStats,
                         classProbs = TRUE,
                         index = index)

## This options tells train() to run it's model tuning
## sequentially. Otherwise, there would be parallel processing at two
## levels, which is possible but requires W^2 workers. On our machine,
## it was more efficient to only run the REF process in parallel.

cvCtrl <- trainControl(method = "cv",
                       verboseIter = FALSE,
                       classProbs = TRUE,
                       allowParallel = FALSE)
```


Fit full model (using all preds) under RFE using train with method = 'sparseLDA'
```{r}
ctrl$functions = caretFuncs
ctrl$functions$summary = fiveStats
set.seed(721)
sparseLDA_full = rfe(training[,predVars], training$Class, sizes = varSeq, rfeControl = ctrl, metric = 'ROC', 
                     #methods for train
                     method = 'sparseLDA', 
                     tuneLength = 5,
                     trControl = cvCtrl)
sparseLDA_full
```


#4
Recall simulation tool from Friedman which utilized non linear equation. Where x1 through x5 have uniform distributions and the error e is normally distributed with mean zero and sd of one. 
a - simulate training set and test set of n =500.  Plot predictors against outcome using scatterplots etc
b - use forward,backward, stepwise algo. Did the final models select the full set of preds? Why or why not. IF CV was used, is the test performance similar to test-set
c - use recursive feature selection with several models
d - apply filter methods where each predictor is evaluated separately and others simultaenously (relief). Were the two interacting predictors(x1 and x2) selected? Was one favored over another
e - reduce sample size and add larger non informative preds. How do these procedures perform under more extreme circumstances

```{r}
library(caret)
library(AppliedPredictiveModeling)
library(mlbench) # has the function friedman1
```

```{r}
set.seed(200)
trainingData = mlbench.friedman1( 500, sd=1 )
trainingData$x = data.frame(trainingData$x)
testingData = mlbench.friedman1( 500, sd=1 )
testingData$x = data.frame(testingData$x)
```


```{r}
library(doParallel)
cl = makeCluster(8)
registerDoParallel(cl)
```

```{r}
featurePlot(trainingData$x, trainingData$y)
```
```{r}
library(RColorBrewer)
library(corrplot)
cols <- c(rev(brewer.pal(7, "Blues")), brewer.pal(7, "Reds"))
corrplot(cor(trainingData$x),order = "hclust",tl.pos = "n",addgrid.col = rgb(1,1,1,.01),col = colorRampPalette(cols)(51))
```
Part b
```{r}
df = trainingData$x; 
df$y = trainingData$y

null = lm(y ~ 1, data = df)
full = lm(y ~., data = df)
```

Selection
```{r}
#forward
step( null, scope=list(lower=null, upper=full), direction="forward", data=df )

#backward
step( full, direction="backward", data=df )

#both 
step( null, scope=list(upper=full), direction="both", data=df )
```
#Use recursive feature selection

```{r}
set.seed(104)
index <- createMultiFolds(trainingData$y, times = 5)

#candidate number of preds
varSeq <- seq(1, dim(trainingData$x)[2], by=1)

#control
ctrl <- rfeControl(method = "repeatedcv", repeats = 5,
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

ctrl$functions <- rfFuncs
```

```{r}
set.seed(721)
rf_rfe = rfe(trainingData$x, trainingData$y, sizes = varSeq, ntree = 500, rfeControl = ctrl)
rf_rfe
```

```{r}
ctrl$functions <- lmFuncs
set.seed(721)
lm_rfe <- rfe(trainingData$x,
             trainingData$y,
             sizes = varSeq,
             tol = 1.0e-12,
             rfeControl = ctrl,
             preProc = c("center", "scale"))
lm_rfe
```

```{r}
# For these models we will also perform cross-validation to select parameters:
cvCtrl <- trainControl(method = "cv",
                       verboseIter = FALSE,
                       classProbs = TRUE,
                       allowParallel = FALSE)
```


```{r}
ctrl$functions <- caretFuncs
set.seed(721)
knn_rfe <- rfe(trainingData$x,
              trainingData$y,
              sizes = varSeq,
              method = "knn",
              tuneLength = 20,
              preProc = c("center", "scale"),
              trControl = cvCtrl,
              rfeControl = ctrl)
knn_rfe
```
```{r}
rownames(rf_rfe$fit[[7]])
```
```{r}
lm_rfe$fit$coefficients
```

Filter methods

Evaluate each predictor separately
```{r}
VI = filterVarImp(trainingData$x, trainingData$y)

library(tidyverse)
VI %>%
  arrange(desc(Overall))
```

Evaluate together and get top 5
```{r}
library(CORElearn)

df = trainingData$x; df$Y = trainingData$y
relief_values = attrEval(Y ~., data = df, estimator = 'RReliefFequalK')
sort(relief_values)
```

Part e - take 100 datapoints and model and see how it performs
```{r}
#
set.seed(201)
trainingData = mlbench.friedman1( 100, sd=1 )
trainingData$x = data.frame(trainingData$x)

df = trainingData$x; df$Y = trainingData$y
null = lm( Y ~ 1, data=df )
full = lm( Y ~ ., data=df )

# Forward selection:
step( null, scope=list(lower=null, upper=full), direction="forward", data=df )

# Backward selection:
step( full, direction="backward", data=df )

# Both directions:
step( null, scope=list(upper=full), direction="both", data=df )
```



Less data points more noise

#5
For cell segmentation data
a - use filter and wrapper methods to determine optimal set of predictords
b - for LDA and logistic, use alternative versions with built in feature selection (glmnet and sparse LDA). how do different approaches compare in terms of performance, number of predictors, and training time


```{r}
library(caret)
library(AppliedPredictiveModeling)
data(segmentationOriginal) # found examples for reading this data in the file: O3_Data_Pre_Processing.R

## Retain the original training set
segTrain <- subset(segmentationOriginal, Case == "Train")

## Remove the first three columns (identifier columns)
segTrainX <- segTrain[, -(1:3)]
segTrainClass <- segTrain$Class
dim(segTrainX)
```
```{r}
zero_cols = nearZeroVar(segTrainX)
segTrainX = segTrainX[,-zero_cols]
```


Remove high correlated predictors
```{r}
high_corr = findCorrelation(cor(segTrainX), cutoff = .75)
segTrainX = segTrainX[,-high_corr]
```

Apply wrapper methods
```{r}
df = segTrainX; df$Class = segTrainClass
null = glm( Class ~ 1, data=df, family=binomial )
full = glm( Class ~ ., data=df, family=binomial )
```
```{r}
forward = step( null, scope=list(lower=null, upper=full), direction="forward", data=df )
```
```{r}
backward = step(full, direction = 'backward', data = df)
names(backward$coefficients)
```
```{r}
both =step( null, scope=list(upper=full), direction="both", data=df )
names(both$coefficients)
```

Using regsubsets function
```{r}
library(leaps)
reg_forward = regsubsets(Class ~ ., data=df, nbest=1, really.big=T, method="forward" )
reg_forward$xnames

reg_backward = regsubsets(Class ~ ., data=df, nbest=1, really.big=T, method="backward" )
reg_backward$xnames
```

stepclass function
```{r}
library(klaR)
sc_forward = stepclass( Class ~ ., data=df, method="lda", direction="forward" )
sc_forward$formula

sc_backward = stepclass( Class ~ ., data=df, method="lda", direction="backward" )
sc_backward$formula

sc_both = stepclass( Class ~ ., data=df, method="lda", direction="both" )
sc_both$formula
```
Apply filter methods
```{r}
VI = filterVarImp( df[,-67], df$Class )
VI
```

```{r}
library(CORElearn)
relief_values = attrEval( Class ~ ., data=df, estimator="Relief" )
print( sort( relief_values, decreasing=TRUE ) )
```
Use recursive feature estimation
```{r}
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))

set.seed(104)
index <- createMultiFolds(df$Class, times = 5)

varSeq <- seq(1, dim(df)[2]-1, by = 2)
```
```{r}
ctrl <- rfeControl(method = "repeatedcv", repeats = 5,
                   saveDetails = TRUE,
                   index = index,
                   returnResamp = "final")

fullCtrl <- trainControl(method = "repeatedcv",
                         repeats = 5,
                         summaryFunction = fiveStats,
                         classProbs = TRUE,
                         index = index)
```

```{r}
ctrl$functions <- rfFuncs
ctrl$functions$summary <- fiveStats
set.seed(721)
rfRFE <- rfe(df[, -67],
             df$Class,
             sizes = varSeq,
             metric = "ROC",
             ntree = 1000,
             rfeControl = ctrl)
```










